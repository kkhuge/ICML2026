import os
import numpy as np
import pickle
from tqdm import tqdm

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
cpath = os.path.dirname(os.path.abspath(__file__))

# ---------------- é…ç½®å‚æ•° ----------------
NUM_USER = 100
SAVE = True
DATASET_ROOT = os.path.join(cpath, "data")  # ä½ çš„æ•°æ®æ ¹ç›®å½•
alpha = 0.1  # Dirichlet å‚æ•°
# ----------------------------------------

np.random.seed(6)


class PathDataset(object):
    """
    åªå­˜å‚¨è·¯å¾„å’Œæ ‡ç­¾çš„ç®€å•åŒ…è£…ç±»
    """

    def __init__(self, image_paths, labels):
        self.data = np.array(image_paths)  # å­˜è·¯å¾„ (String)
        self.target = np.array(labels)  # å­˜æ ‡ç­¾ (Int)

    def __len__(self):
        return len(self.target)


# -------------------------------------------------------------
# åŠ è½½ Tiny-ImageNet (åªè¯»å–è·¯å¾„ï¼Œä¸è¯»å–å›¾ç‰‡)
# -------------------------------------------------------------
def load_tinyimagenet_paths(dataset_root):
    # å°è¯•è‡ªåŠ¨é€‚é…ä¸¤ç§å¸¸è§è·¯å¾„
    if os.path.exists(os.path.join(dataset_root, "tiny-imagenet-200")):
        real_root = os.path.join(dataset_root, "tiny-imagenet-200")
    else:
        real_root = dataset_root

    train_dir = os.path.join(real_root, "train")
    val_dir = os.path.join(real_root, "val")
    wnids_path = os.path.join(real_root, "wnids.txt")

    # 1. åŠ è½½ç±»åˆ«æ˜ å°„
    if not os.path.exists(wnids_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ° wnids.txt, è¯·æ£€æŸ¥è·¯å¾„: {wnids_path}")

    with open(wnids_path, "r") as f:
        wnids = [w.strip() for w in f.readlines()]
    wnid_to_label = {wnid: idx for idx, wnid in enumerate(wnids)}

    # 2. è¯»å–è®­ç»ƒé›†è·¯å¾„
    print(">>> Collecting Train Paths...")
    train_paths, train_labels = [], []

    for wnid in tqdm(wnids, desc="Scanning Train"):
        image_folder = os.path.join(train_dir, wnid, "images")
        if not os.path.exists(image_folder):
            continue

        label = wnid_to_label[wnid]
        fnames = [f for f in os.listdir(image_folder) if f.endswith('.JPEG')]

        for fname in fnames:
            path = os.path.join(image_folder, fname)
            train_paths.append(path)
            train_labels.append(label)

    # 3. è¯»å–éªŒè¯é›†è·¯å¾„
    print(">>> Collecting Val Paths...")
    val_paths, val_labels = [], []
    anno_path = os.path.join(val_dir, "val_annotations.txt")
    val_img_dir = os.path.join(val_dir, "images")

    with open(anno_path, "r") as f:
        lines = f.readlines()

    for line in lines:
        parts = line.split()
        fname = parts[0]
        wnid = parts[1]

        label = wnid_to_label[wnid]
        path = os.path.join(val_img_dir, fname)

        val_paths.append(path)
        val_labels.append(label)

    print(f"Loaded {len(train_paths)} train images, {len(val_paths)} val images.")

    return PathDataset(train_paths, train_labels), \
        PathDataset(val_paths, val_labels), \
        len(wnids)


# -------------------------------------------------------------
# ä¸»å‡½æ•°
# -------------------------------------------------------------
def main():
    print(f">>> Root: {DATASET_ROOT}")

    # 1. åŠ è½½æ•°æ®
    trainset, testset, num_classes = load_tinyimagenet_paths(DATASET_ROOT)

    # 2. ç”Ÿæˆ Dirichlet åˆ†å¸ƒ
    print(f">>> Generating Non-IID distribution (alpha={alpha})...")
    min_size = 0
    K = num_classes
    N = len(trainset)

    # === å®‰å…¨ä¿æŠ¤ ===
    cnt = 0
    while min_size < 10:
        if cnt > 50:
            print(">>> Warning: Random generation timed out. Breaking loop.")
            break
        cnt += 1

        idx_batch = [[] for _ in range(NUM_USER)]
        for k in range(K):
            idx_k = np.where(trainset.target == k)[0]
            np.random.shuffle(idx_k)
            proportions = np.random.dirichlet(np.repeat(alpha, NUM_USER))
            proportions = np.array([p * (len(idx_j) < N / NUM_USER) for p, idx_j in zip(proportions, idx_batch)])
            proportions = proportions / proportions.sum()
            proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]
            idx_batch_split = np.split(idx_k, proportions)
            for u in range(NUM_USER):
                idx_batch[u] += idx_batch_split[u].tolist()

        min_size = min([len(idx_j) for idx_j in idx_batch])
        print(f"    - Try {cnt}: Min size: {min_size}")

    # 3. åˆ†é…æ•°æ®
    train_data = {'users': [], 'user_data': {}, 'num_samples': []}
    test_data = {'users': [], 'user_data': {}, 'num_samples': []}

    # æµ‹è¯•é›†ç´¢å¼•åˆ‡åˆ† (Uniform)
    all_test_idxs = np.arange(len(testset))
    np.random.shuffle(all_test_idxs)
    test_idxs_split = np.array_split(all_test_idxs, NUM_USER)

    # ============================================================
    # ğŸŸ¢ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ•´æ•° i ä½œä¸º User IDï¼Œè€Œä¸æ˜¯ str(i)
    # ============================================================
    print(">>> Allocating Data (Using Integer User IDs)...")

    for i in range(NUM_USER):
        uname = i  # âœ… è¿™é‡Œç›´æ¥ä½¿ç”¨æ•´æ•° (int)

        # --- è®­ç»ƒé›† ---
        train_idxs = idx_batch[i]
        train_data['users'].append(uname)
        train_data['user_data'][uname] = {
            'x': trainset.data[train_idxs].tolist(),
            'y': trainset.target[train_idxs].tolist()
        }
        train_data['num_samples'].append(len(train_idxs))

        # --- æµ‹è¯•é›† ---
        test_idxs = test_idxs_split[i]
        test_data['users'].append(uname)
        test_data['user_data'][uname] = {
            'x': testset.data[test_idxs].tolist(),
            'y': testset.target[test_idxs].tolist()
        }
        test_data['num_samples'].append(len(test_idxs))

    # ============================================================

    # 4. ä¿å­˜
    if SAVE:
        print('>>> Saving data to pkl...')

        common_filename = f'tinyimagenet_niid_{alpha}.pkl'

        train_path = os.path.join(cpath, 'data', 'train', common_filename)
        test_path = os.path.join(cpath, 'data', 'test', common_filename)

        os.makedirs(os.path.dirname(train_path), exist_ok=True)
        os.makedirs(os.path.dirname(test_path), exist_ok=True)

        with open(train_path, "wb") as f:
            pickle.dump(train_data, f)

        with open(test_path, "wb") as f:
            pickle.dump(test_data, f)

        print(">>> Save Done.")
        print(f"    Train file: {train_path}")
        print(f"    Test file:  {test_path}")
        print(f"    New Dataset Name for main.py: tinyimagenet_niid_{alpha}")


if __name__ == '__main__':
    main()
